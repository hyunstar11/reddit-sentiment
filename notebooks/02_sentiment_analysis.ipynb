{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Running the full annotation pipeline (VADER + optional transformer)\n",
    "2. Exploring sentiment distributions\n",
    "3. Brand detection and context extraction examples\n",
    "4. Purchase intent signals\n",
    "\n",
    "> **Prerequisites:** `data/raw/posts_*.parquet` must exist (run `make collect` first).  \n",
    "> For transformer scoring: `uv sync --extra ml` then re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_sentiment.collection.collector import SubredditCollector\n",
    "\n",
    "raw_dir = Path('../data/raw')\n",
    "try:\n",
    "    raw_df = SubredditCollector.load_latest(raw_dir)\n",
    "    print(f'Loaded {len(raw_df):,} raw records')\n",
    "except FileNotFoundError:\n",
    "    # Demo synthetic data\n",
    "    raw_df = pd.DataFrame({\n",
    "        'id': [f'r{i}' for i in range(200)],\n",
    "        'subreddit': ['Sneakers'] * 100 + ['Nike'] * 60 + ['Adidas'] * 40,\n",
    "        'record_type': ['post'] * 120 + ['comment'] * 80,\n",
    "        'score': [50] * 200,\n",
    "        'created_utc': pd.date_range('2024-01-01', periods=200, freq='3H', tz='UTC'),\n",
    "        'full_text': [\n",
    "            'I just copped the Nike Air Max and they are amazing quality!',\n",
    "            'Adidas Yeezy resale prices are insane, way too expensive',\n",
    "            'Where to cop the New Balance 990v4? W2C?',\n",
    "            'Hoka Clifton runs are so comfortable for marathon training',\n",
    "            'The Three Stripes collab with Pharrell is fire',\n",
    "        ] * 40,\n",
    "        'extracted_urls': [['https://stockx.com/buy/nike-air-max']] * 100 + [[]] * 100,\n",
    "    })\n",
    "    print('Using synthetic demo data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Annotation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_sentiment.sentiment.pipeline import SentimentPipeline\n",
    "\n",
    "# Use VADER-only mode (no transformer download needed)\n",
    "# Set use_transformer=True if you have ML extras installed\n",
    "pipeline = SentimentPipeline(use_transformer=False)\n",
    "\n",
    "annotated = pipeline.annotate(raw_df)\n",
    "print(f'Annotated {len(annotated):,} records')\n",
    "print(f'New columns: {[c for c in annotated.columns if c not in raw_df.columns]}')\n",
    "annotated[['id', 'vader_score', 'hybrid_score', 'brands', 'channels', 'primary_intent']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    annotated, x='hybrid_score', nbins=40,\n",
    "    title='Hybrid Sentiment Score Distribution',\n",
    "    labels={'hybrid_score': 'Sentiment Score (-1 to +1)', 'count': 'Records'},\n",
    "    color_discrete_sequence=['#4f46e5']\n",
    ")\n",
    "fig.add_vline(x=0, line_dash='dash', line_color='gray')\n",
    "fig.show()\n",
    "\n",
    "print(f\"Mean sentiment: {annotated['hybrid_score'].mean():.4f}\")\n",
    "print(f\"Positive (>0.05): {(annotated['hybrid_score'] > 0.05).mean()*100:.1f}%\")\n",
    "print(f\"Negative (<-0.05): {(annotated['hybrid_score'] < -0.05).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Brand Detection Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_sentiment.detection.brands import BrandDetector\n",
    "\n",
    "detector = BrandDetector(context_window=10)\n",
    "\n",
    "examples = [\n",
    "    'The Three Stripes collab with Beyoncé is unmatched',\n",
    "    'Way of Wade 10 just dropped and the colorway is fire',\n",
    "    'UA Curry shoes are underrated for basketball courts',\n",
    "    'NB 990v4 vs Nike Dunk — which is the better retro?',\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    brands = detector.detect_brands(text)\n",
    "    print(f'Text: \"{text}\"')\n",
    "    print(f'  → Brands: {brands}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brands by mention count\n",
    "brand_counts = annotated.explode('brands')['brands'].value_counts().reset_index()\n",
    "brand_counts.columns = ['brand', 'mentions']\n",
    "brand_counts = brand_counts[brand_counts['brand'].notna() & (brand_counts['brand'] != '')]\n",
    "\n",
    "fig = px.bar(\n",
    "    brand_counts.head(10), x='brand', y='mentions',\n",
    "    title='Brand Mention Frequency',\n",
    "    color='mentions', color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Purchase Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_counts = annotated['primary_intent'].value_counts().reset_index()\n",
    "intent_counts.columns = ['intent', 'count']\n",
    "intent_counts = intent_counts[intent_counts['intent'].notna()]\n",
    "\n",
    "fig = px.funnel(\n",
    "    intent_counts.head(7),\n",
    "    x='count', y='intent',\n",
    "    title='Purchase Intent Signal Distribution'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"Records with any intent signal: {annotated['primary_intent'].notna().sum():,}\")\n",
    "print(f\"Records without intent: {annotated['primary_intent'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment by intent type\n",
    "intent_sentiment = (\n",
    "    annotated[annotated['primary_intent'].notna()]\n",
    "    .groupby('primary_intent')['hybrid_score']\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "intent_sentiment.columns = ['intent', 'avg_sentiment']\n",
    "\n",
    "colours = ['#ef4444' if s < 0 else '#22c55e' for s in intent_sentiment['avg_sentiment']]\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=intent_sentiment['avg_sentiment'],\n",
    "    y=intent_sentiment['intent'],\n",
    "    orientation='h',\n",
    "    marker_color=colours\n",
    "))\n",
    "fig.update_layout(title='Average Sentiment by Intent Type', xaxis=dict(range=[-1, 1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Annotated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path('../data/processed/annotated.parquet')\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "annotated.to_parquet(out_path, index=False)\n",
    "print(f'Saved: {out_path}')"
   ]
  }
 ],
 "metadata": {\n",
  "kernelspec": {\n",
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n",
  "language_info": {\n",
   "name": "python",\n",
   "version": "3.11.0"\n",
  }\n",
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
