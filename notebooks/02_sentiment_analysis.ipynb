{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Running the full annotation pipeline (VADER + optional transformer)\n",
    "2. Exploring sentiment distributions\n",
    "3. Brand detection and context extraction examples\n",
    "4. Purchase intent signals\n",
    "\n",
    "> **Prerequisites:** `data/raw/posts_*.parquet` must exist (run `make collect` first).  \n",
    "> For transformer scoring: `uv sync --extra ml` then re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_sentiment.collection.collector import SubredditCollector\n",
    "\n",
    "raw_dir = Path('../data/raw')\n",
    "try:\n",
    "    raw_df = SubredditCollector.load_latest(raw_dir)\n",
    "    print(f'Loaded {len(raw_df):,} raw records')\n",
    "except FileNotFoundError:\n",
    "    # Demo synthetic data\n",
    "    raw_df = pd.DataFrame({\n",
    "        'id': [f'r{i}' for i in range(200)],\n",
    "        'subreddit': ['Sneakers'] * 100 + ['Nike'] * 60 + ['Adidas'] * 40,\n",
    "        'record_type': ['post'] * 120 + ['comment'] * 80,\n",
    "        'score': [50] * 200,\n",
    "        'created_utc': pd.date_range('2024-01-01', periods=200, freq='3H', tz='UTC'),\n",
    "        'full_text': [\n",
    "            'I just copped the Nike Air Max and they are amazing quality!',\n",
    "            'Adidas Yeezy resale prices are insane, way too expensive',\n",
    "            'Where to cop the New Balance 990v4? W2C?',\n",
    "            'Hoka Clifton runs are so comfortable for marathon training',\n",
    "            'The Three Stripes collab with Pharrell is fire',\n",
    "        ] * 40,\n",
    "        'extracted_urls': [['https://stockx.com/buy/nike-air-max']] * 100 + [[]] * 100,\n",
    "    })\n",
    "    print('Using synthetic demo data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Annotation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_sentiment.sentiment.pipeline import SentimentPipeline\n",
    "\n",
    "# Use VADER-only mode (no transformer download needed)\n",
    "# Set use_transformer=True if you have ML extras installed\n",
    "pipeline = SentimentPipeline(use_transformer=False)\n",
    "\n",
    "annotated = pipeline.annotate(raw_df)\n",
    "print(f'Annotated {len(annotated):,} records')\n",
    "print(f'New columns: {[c for c in annotated.columns if c not in raw_df.columns]}')\n",
    "annotated[['id', 'vader_score', 'hybrid_score', 'brands', 'channels', 'primary_intent']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    annotated, x='hybrid_score', nbins=40,\n",
    "    title='Hybrid Sentiment Score Distribution',\n",
    "    labels={'hybrid_score': 'Sentiment Score (-1 to +1)', 'count': 'Records'},\n",
    "    color_discrete_sequence=['#4f46e5']\n",
    ")\n",
    "fig.add_vline(x=0, line_dash='dash', line_color='gray')\n",
    "fig.show()\n",
    "\n",
    "print(f\"Mean sentiment: {annotated['hybrid_score'].mean():.4f}\")\n",
    "print(f\"Positive (>0.05): {(annotated['hybrid_score'] > 0.05).mean()*100:.1f}%\")\n",
    "print(f\"Negative (<-0.05): {(annotated['hybrid_score'] < -0.05).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Brand Detection Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_sentiment.detection.brands import BrandDetector\n",
    "\n",
    "detector = BrandDetector(context_window=10)\n",
    "\n",
    "examples = [\n",
    "    'The Three Stripes collab with Beyonc\u00e9 is unmatched',\n",
    "    'Way of Wade 10 just dropped and the colorway is fire',\n",
    "    'UA Curry shoes are underrated for basketball courts',\n",
    "    'NB 990v4 vs Nike Dunk \u2014 which is the better retro?',\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    brands = detector.detect_brands(text)\n",
    "    print(f'Text: \"{text}\"')\n",
    "    print(f'  \u2192 Brands: {brands}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brands by mention count\n",
    "brand_counts = annotated.explode('brands')['brands'].value_counts().reset_index()\n",
    "brand_counts.columns = ['brand', 'mentions']\n",
    "brand_counts = brand_counts[brand_counts['brand'].notna() & (brand_counts['brand'] != '')]\n",
    "\n",
    "fig = px.bar(\n",
    "    brand_counts.head(10), x='brand', y='mentions',\n",
    "    title='Brand Mention Frequency',\n",
    "    color='mentions', color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Purchase Intent Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_counts = annotated['primary_intent'].value_counts().reset_index()\n",
    "intent_counts.columns = ['intent', 'count']\n",
    "intent_counts = intent_counts[intent_counts['intent'].notna()]\n",
    "\n",
    "fig = px.funnel(\n",
    "    intent_counts.head(7),\n",
    "    x='count', y='intent',\n",
    "    title='Purchase Intent Signal Distribution'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"Records with any intent signal: {annotated['primary_intent'].notna().sum():,}\")\n",
    "print(f\"Records without intent: {annotated['primary_intent'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment by intent type\n",
    "intent_sentiment = (\n",
    "    annotated[annotated['primary_intent'].notna()]\n",
    "    .groupby('primary_intent')['hybrid_score']\n",
    "    .mean()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "intent_sentiment.columns = ['intent', 'avg_sentiment']\n",
    "\n",
    "colours = ['#ef4444' if s < 0 else '#22c55e' for s in intent_sentiment['avg_sentiment']]\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=intent_sentiment['avg_sentiment'],\n",
    "    y=intent_sentiment['intent'],\n",
    "    orientation='h',\n",
    "    marker_color=colours\n",
    "))\n",
    "fig.update_layout(title='Average Sentiment by Intent Type', xaxis=dict(range=[-1, 1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bd894",
   "metadata": {},
   "source": [
    "## 6. Posts vs Comments: Sentiment Comparison\n",
    "\n",
    "Reddit comments tend to be more emotionally reactive than posts. Posts are often structured questions or reviews; comments are rapid, context-driven responses."
   ]
  },
  {
   "cell_type": "code",
   "id": "0fb24a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'record_type' in annotated.columns and annotated['record_type'].nunique() > 1:\n",
    "    # Distribution overlay\n",
    "    fig = px.histogram(\n",
    "        annotated, x='hybrid_score', color='record_type',\n",
    "        nbins=40, barmode='overlay', opacity=0.75,\n",
    "        title='Sentiment Distribution: Posts vs Comments',\n",
    "        labels={'hybrid_score': 'Hybrid Sentiment Score', 'record_type': 'Type'},\n",
    "        color_discrete_map={'post': '#4f46e5', 'comment': '#06b6d4'},\n",
    "    )\n",
    "    fig.add_vline(x=0, line_dash='dash', line_color='gray')\n",
    "    fig.show()\n",
    "\n",
    "    # Summary table\n",
    "    comparison = (\n",
    "        annotated.groupby('record_type')['hybrid_score']\n",
    "        .agg(count='count', mean='mean', std='std')\n",
    "        .rename(columns={'count': 'records', 'mean': 'avg_sentiment', 'std': 'std_dev'})\n",
    "        .round(4)\n",
    "    )\n",
    "    print(\"Sentiment summary by record type:\")\n",
    "    display(comparison)\n",
    "else:\n",
    "    print(\"Only one record type present \u2014 run 'reddit-sentiment collect --public' to include comments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224f171",
   "metadata": {},
   "source": [
    "## 7. Sentiment by Subreddit\n",
    "\n",
    "Different subreddits have different emotional baselines. r/Jordans skews positive (brand fans); r/SneakerMarket is more neutral (transactional). Comparing subreddit sentiment helps separate platform bias from genuine brand signal."
   ]
  },
  {
   "cell_type": "code",
   "id": "b278d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'subreddit' in annotated.columns:\n",
    "    sub_sent = (\n",
    "        annotated.groupby('subreddit')['hybrid_score']\n",
    "        .agg(avg_sentiment='mean', records='count')\n",
    "        .sort_values('avg_sentiment', ascending=False)\n",
    "        .reset_index()\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "    colours = ['#22c55e' if s > 0.05 else '#ef4444' if s < -0.05 else '#94a3b8'\n",
    "               for s in sub_sent['avg_sentiment']]\n",
    "\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=sub_sent['avg_sentiment'],\n",
    "        y=sub_sent['subreddit'],\n",
    "        orientation='h',\n",
    "        marker_color=colours,\n",
    "        text=[f\"{s:+.3f} ({n:,} records)\" for s, n in\n",
    "              zip(sub_sent['avg_sentiment'], sub_sent['records'])],\n",
    "        textposition='outside',\n",
    "    ))\n",
    "    fig.add_vline(x=0, line_dash='dash', line_color='gray')\n",
    "    fig.update_layout(\n",
    "        title='Average Sentiment by Subreddit',\n",
    "        xaxis=dict(range=[-0.5, 0.7]),\n",
    "        height=400, plot_bgcolor='white',\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f2dc3",
   "metadata": {},
   "source": [
    "## 8. Shoe Model Detection\n",
    "\n",
    "Beyond brands, the pipeline identifies specific shoe models (Air Jordan 1, Dunk Low, Yeezy 350, etc.) using alias-based pattern matching. This powers the price correlation analysis in Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "id": "41cd7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_sentiment.detection.models import ModelDetector\n",
    "\n",
    "detector = ModelDetector()\n",
    "\n",
    "examples = [\n",
    "    'Just picked up the AJ1 Bred Toe \u2014 worth every penny at retail',\n",
    "    'Dunk Low pandas are finally restocking on Nike SNKRS',\n",
    "    'My 990v6 is the comfiest shoe I have ever run in, beats UB23',\n",
    "    'Yeezy 350 V2 Zebra at retail? No way, minimum 2x resale',\n",
    "]\n",
    "\n",
    "print(\"Model detection examples:\\n\")\n",
    "for text in examples:\n",
    "    models = detector.detect_models(text)\n",
    "    mentions = detector.detect(text)\n",
    "    print(f'  Text: \"{text}\"')\n",
    "    for m in mentions:\n",
    "        print(f'    \u2192 {m.model} (matched alias: \"{m.alias}\", retail: ${m.retail_price:.0f})')\n",
    "    print()\n",
    "\n",
    "# Models found in the annotated dataset\n",
    "if 'models' in annotated.columns:\n",
    "    from collections import Counter\n",
    "    all_models = [m for models in annotated['models'].dropna() for m in (models if isinstance(models, list) else [])]\n",
    "    model_counts = pd.DataFrame(Counter(all_models).most_common(15), columns=['model', 'mentions'])\n",
    "    model_counts = model_counts[model_counts['model'].astype(str).str.strip().ne('')]\n",
    "    if not model_counts.empty:\n",
    "        fig = px.bar(\n",
    "            model_counts, x='mentions', y='model', orientation='h',\n",
    "            title='Top Shoe Models by Reddit Mentions',\n",
    "            color='mentions', color_continuous_scale='Purples',\n",
    "        )\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"No shoe model detections found in annotated data \u2014 ensure full_text column is populated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Annotated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path('../data/processed/annotated.parquet')\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "annotated.to_parquet(out_path, index=False)\n",
    "print(f'Saved: {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}