{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Collection & Exploratory Data Analysis\n",
    "\n",
    "This notebook walks through:\n",
    "1. How Reddit data is collected via PRAW\n",
    "2. Loading and inspecting the raw Parquet files\n",
    "3. EDA: post volume, score distributions, subreddit breakdown\n",
    "\n",
    "> **Before running:** ensure `REDDIT_CLIENT_ID` and `REDDIT_CLIENT_SECRET` are set in `.env`,  \n",
    "> then run `make collect` (or the cell below) to fetch live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Collection Architecture\n\nTwo collectors are available \u2014 both write the same Parquet schema:\n\n| Collector | Credentials | Speed | Comments |\n|-----------|-------------|-------|----------|\n| `SubredditCollector` (PRAW) | Reddit API key required | Faster, full metadata | \u2705 Yes (50/post) |\n| `PublicSubredditCollector` | None needed | Slightly slower (~10 min) | \u2705 Yes (top 20 posts/subreddit) |\n\n```\nPublicSubredditCollector\n  \u251c\u2500\u2500 r/Sneakers, r/Nike, r/Adidas, r/Jordans, \u2026  (9 subreddits)\n  \u251c\u2500\u2500 sort: hot / top / new  \u2192 up to ~167 posts each\n  \u2514\u2500\u2500 comments: top 20 posts \u00d7 50 comments = up to 1,000 comments/subreddit\n```\n\nRun `reddit-sentiment collect --public` (default, no credentials) or `reddit-sentiment collect` (PRAW)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally run collection (takes 10-30 min for full dataset)\n",
    "# Uncomment if you want to fetch fresh data:\n",
    "\n",
    "# from reddit_sentiment.collection.collector import SubredditCollector\n",
    "# collector = SubredditCollector()\n",
    "# raw_path = collector.collect()\n",
    "# print(f'Collected data saved to: {raw_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the most recent raw data file \u2014 tries PRAW collector glob first,\n# then falls back to synthetic demo data\nfrom reddit_sentiment.collection.collector import SubredditCollector\n\nraw_dir = Path('../data/raw')\ntry:\n    df = SubredditCollector.load_latest(raw_dir)\n    print(f'Loaded {len(df):,} records from {raw_dir}')\n    print(f'Columns: {list(df.columns)}')\nexcept FileNotFoundError:\n    print('No raw data found. Run: reddit-sentiment collect --public')\n    df = pd.DataFrame({\n        'id': [f'post_{i}' for i in range(200)],\n        'subreddit': ['Sneakers'] * 80 + ['Nike'] * 50 + ['Adidas'] * 40 + ['Jordans'] * 30,\n        'record_type': ['post'] * 120 + ['comment'] * 80,\n        'score': [100, 50, 200, 10, 30] * 40,\n        'num_comments': [5, 10, 2, 20, 8] * 40,\n        'created_utc': pd.date_range('2025-01-01', periods=200, freq='6H', tz='UTC'),\n        'full_text': ['Nike Air Jordan review \u2014 great quality and comfortable fit'] * 200,\n        'extracted_urls': [[]] * 200,\n    })\n    print(f'Using {len(df):,}-row synthetic demo data.')\n\ndf.head(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total records: {len(df):,}\")\n",
    "if 'record_type' in df.columns:\n",
    "    print(df['record_type'].value_counts().to_string())\n",
    "print(f\"\\nSubreddits: {df['subreddit'].nunique()}\")\n",
    "print(df['subreddit'].value_counts().head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Posts vs Comments breakdown\nif 'record_type' in df.columns:\n    type_counts = df['record_type'].value_counts()\n    posts = type_counts.get('post', 0)\n    comments = type_counts.get('comment', 0)\n    total = len(df)\n    print(f\"Posts:    {posts:,}  ({posts/total*100:.1f}%)\")\n    print(f\"Comments: {comments:,}  ({comments/total*100:.1f}%)\")\n    print(f\"Total:    {total:,}\")\n    print()\n    # Comment-to-post ratio\n    if posts > 0:\n        print(f\"Avg comments fetched per post: {comments/posts:.1f}\")\n\n    fig = px.pie(\n        values=type_counts.values,\n        names=type_counts.index,\n        title='Posts vs Comments',\n        color_discrete_map={'post': '#4f46e5', 'comment': '#06b6d4'},\n        hole=0.4,\n    )\n    fig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post score distribution\n",
    "if 'score' in df.columns:\n",
    "    posts = df[df.get('record_type', 'post') == 'post'] if 'record_type' in df.columns else df\n",
    "    fig = px.histogram(\n",
    "        posts, x='score', nbins=50, log_y=True,\n",
    "        title='Distribution of Post Scores (log scale)',\n",
    "        labels={'score': 'Reddit Score', 'count': 'Number of Posts'},\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records per subreddit\n",
    "sub_counts = df['subreddit'].value_counts().reset_index()\n",
    "sub_counts.columns = ['subreddit', 'count']\n",
    "\n",
    "fig = px.bar(\n",
    "    sub_counts.head(10),\n",
    "    x='subreddit', y='count',\n",
    "    title='Records per Subreddit',\n",
    "    color='count',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "id": "61e62c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts over time\n",
    "if 'created_utc' in df.columns:\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], utc=True, errors='coerce')\n",
    "    df['date'] = df['created_utc'].dt.date\n",
    "    daily = df.groupby('date').size().reset_index(name='count')\n",
    "\n",
    "    fig = px.line(daily, x='date', y='count', title='Daily Post/Comment Volume')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af6688",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis\n",
    "\n",
    "Comments and posts have different length profiles. Posts tend to have longer, more deliberate text; comments are shorter and more reactive. Both contribute differently to the sentiment signal."
   ]
  },
  {
   "cell_type": "code",
   "id": "a0d3fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution: posts vs comments\n",
    "if 'full_text' in df.columns:\n",
    "    df['text_len'] = df['full_text'].str.len().fillna(0).astype(int)\n",
    "\n",
    "    if 'record_type' in df.columns:\n",
    "        fig = px.histogram(\n",
    "            df, x='text_len', color='record_type', nbins=60, barmode='overlay',\n",
    "            title='Text Length Distribution: Posts vs Comments',\n",
    "            labels={'text_len': 'Character count', 'count': 'Records'},\n",
    "            color_discrete_map={'post': '#4f46e5', 'comment': '#06b6d4'},\n",
    "            opacity=0.7,\n",
    "        )\n",
    "    else:\n",
    "        fig = px.histogram(\n",
    "            df, x='text_len', nbins=60,\n",
    "            title='Text Length Distribution',\n",
    "            labels={'text_len': 'Character count'},\n",
    "        )\n",
    "    fig.update_layout(xaxis_range=[0, 2000])\n",
    "    fig.show()\n",
    "\n",
    "    # Summary stats by type\n",
    "    len_stats = df.groupby('record_type')['text_len'].describe()[['mean','50%','max']].round(0)\n",
    "    len_stats.columns = ['Mean chars', 'Median chars', 'Max chars']\n",
    "    display(len_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76fd4e",
   "metadata": {},
   "source": [
    "## 4. URL & Channel Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'extracted_urls' in df.columns:\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    all_urls = [url for urls in df['extracted_urls'] if isinstance(urls, list) for url in urls]\n",
    "    domains = [urlparse(u).netloc.lower().lstrip('www.') for u in all_urls if u]\n",
    "    \n",
    "    from collections import Counter\n",
    "    top_domains = pd.DataFrame(Counter(domains).most_common(15), columns=['domain', 'count'])\n",
    "    print(f'Total URLs found: {len(all_urls):,}')\n",
    "    print(f'Unique domains: {len(set(domains)):,}')\n",
    "    display(top_domains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}