{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Collection & Exploratory Data Analysis\n",
    "\n",
    "This notebook walks through:\n",
    "1. How Reddit data is collected via PRAW\n",
    "2. Loading and inspecting the raw Parquet files\n",
    "3. EDA: post volume, score distributions, subreddit breakdown\n",
    "\n",
    "> **Before running:** ensure `REDDIT_CLIENT_ID` and `REDDIT_CLIENT_SECRET` are set in `.env`,  \n",
    "> then run `make collect` (or the cell below) to fetch live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collection Architecture\n",
    "\n",
    "The `SubredditCollector` fetches posts and comments from 9 sneaker subreddits using three sort methods (`hot`, `top`, `new`) and writes the result to Parquet with a crash-recovery checkpoint.\n",
    "\n",
    "```\n",
    "SubredditCollector\n",
    "  ├── sort: hot  → up to 167 posts\n",
    "  ├── sort: top  → up to 167 posts (last month)\n",
    "  └── sort: new  → up to 167 posts\n",
    "       └── per post: up to 50 comments\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally run collection (takes 10-30 min for full dataset)\n",
    "# Uncomment if you want to fetch fresh data:\n",
    "\n",
    "# from reddit_sentiment.collection.collector import SubredditCollector\n",
    "# collector = SubredditCollector()\n",
    "# raw_path = collector.collect()\n",
    "# print(f'Collected data saved to: {raw_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent raw data file\n",
    "from reddit_sentiment.collection.collector import SubredditCollector\n",
    "\n",
    "raw_dir = Path('../data/raw')\n",
    "try:\n",
    "    df = SubredditCollector.load_latest(raw_dir)\n",
    "    print(f'Loaded {len(df):,} records')\n",
    "    print(f'Columns: {list(df.columns)}')\n",
    "except FileNotFoundError:\n",
    "    print('No raw data found. Run `make collect` first.')\n",
    "    # Create a small synthetic demo dataset\n",
    "    df = pd.DataFrame({\n",
    "        'id': [f'post_{i}' for i in range(100)],\n",
    "        'subreddit': ['Sneakers'] * 50 + ['Nike'] * 30 + ['Adidas'] * 20,\n",
    "        'record_type': ['post'] * 60 + ['comment'] * 40,\n",
    "        'score': [100, 50, 200, 10, 30] * 20,\n",
    "        'num_comments': [5, 10, 2, 20, 8] * 20,\n",
    "        'created_utc': pd.date_range('2024-01-01', periods=100, freq='6H', tz='UTC'),\n",
    "        'full_text': ['Nike Air Max review'] * 100,\n",
    "        'extracted_urls': [[]] * 100,\n",
    "    })\n",
    "    print('Using synthetic demo data.')\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total records: {len(df):,}\")\n",
    "if 'record_type' in df.columns:\n",
    "    print(df['record_type'].value_counts().to_string())\n",
    "print(f\"\\nSubreddits: {df['subreddit'].nunique()}\")\n",
    "print(df['subreddit'].value_counts().head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post score distribution\n",
    "if 'score' in df.columns:\n",
    "    posts = df[df.get('record_type', 'post') == 'post'] if 'record_type' in df.columns else df\n",
    "    fig = px.histogram(\n",
    "        posts, x='score', nbins=50, log_y=True,\n",
    "        title='Distribution of Post Scores (log scale)',\n",
    "        labels={'score': 'Reddit Score', 'count': 'Number of Posts'},\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Records per subreddit\n",
    "sub_counts = df['subreddit'].value_counts().reset_index()\n",
    "sub_counts.columns = ['subreddit', 'count']\n",
    "\n",
    "fig = px.bar(\n",
    "    sub_counts.head(10),\n",
    "    x='subreddit', y='count',\n",
    "    title='Records per Subreddit',\n",
    "    color='count',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts over time\n",
    "if 'created_utc' in df.columns:\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], utc=True, errors='coerce')\n",
    "    df['date'] = df['created_utc'].dt.date\n",
    "    daily = df.groupby('date').size().reset_index(name='count')\n",
    "    \n",
    "    fig = px.line(daily, x='date', y='count', title='Daily Post/Comment Volume')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. URL & Channel Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'extracted_urls' in df.columns:\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    all_urls = [url for urls in df['extracted_urls'] if isinstance(urls, list) for url in urls]\n",
    "    domains = [urlparse(u).netloc.lower().lstrip('www.') for u in all_urls if u]\n",
    "    \n",
    "    from collections import Counter\n",
    "    top_domains = pd.DataFrame(Counter(domains).most_common(15), columns=['domain', 'count'])\n",
    "    print(f'Total URLs found: {len(all_urls):,}')\n",
    "    print(f'Unique domains: {len(set(domains)):,}')\n",
    "    display(top_domains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
